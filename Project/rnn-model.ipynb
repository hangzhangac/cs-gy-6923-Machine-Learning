{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate\n",
    "## We recommend you to reach above websites and click the \"new notebook\" inside, turn on the TPU accelerator.  The Kaggle enviromnet would be preferable to run our code. If you want to run our code locally, please download the dataset from above website, and change the first few lines in code to refer to your directory with local dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/60k-stack-overflow-questions-with-quality-rate/valid.csv\n",
      "/kaggle/input/60k-stack-overflow-questions-with-quality-rate/train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=7, micro=10, releaselevel='final', serial=0)\n",
      "matplotlib 3.4.1\n",
      "numpy 1.19.5\n",
      "pandas 1.2.3\n",
      "sklearn 0.24.1\n",
      "tensorflow 2.4.1\n",
      "tensorflow.keras 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import re\n",
    "\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'Title', 'Body', 'Tags', 'CreationDate', 'Y'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_data1 = pd.read_csv(r'/kaggle/input/60k-stack-overflow-questions-with-quality-rate/valid.csv')\n",
    "stack_data2 = pd.read_csv(r'/kaggle/input/60k-stack-overflow-questions-with-quality-rate/train.csv')\n",
    "stack_data = pd.concat([stack_data1,stack_data2])\n",
    "stack_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the NAN(useless information) from dataset\n",
    "stack_data_f = stack_data.dropna(subset=['Y'])\n",
    "# Combine the Title and Body part of comment as the input X \n",
    "stack_data_f['text'] = stack_data_f.Title +': '+ stack_data_f.Body + stack_data_f.Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the label into one-hot vector\n",
    "Y = stack_data_f.Y\n",
    "format_new = {'LQ_CLOSE':0, 'HQ':1, 'LQ_EDIT':2}\n",
    "Y = np.array(Y.apply(lambda x: format_new[x]))\n",
    "\n",
    "Y_onehot = []\n",
    "formatq = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "for y in Y:\n",
    "    Y_onehot.append(formatq[y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Split the train and test dataset with test-size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(stack_data_f.text, np.array(Y_onehot), test_size=0.2, random_state=0 )\n",
    "\n",
    "maxfeatures = 20000 # Use 20000 words with most frequnences\n",
    "\n",
    "\n",
    "# Use tokenizer to encode the data from text to number\n",
    "tok = tf.keras.preprocessing.text.Tokenizer(num_words = maxfeatures)\n",
    "tok.fit_on_texts(stack_data_f.text)\n",
    "X_train = tok.texts_to_sequences(X_train)\n",
    "X_test = tok.texts_to_sequences(X_test)\n",
    "\n",
    "# Use the mean length of data as the restriction length\n",
    "sentence_all = tok.texts_to_sequences(stack_data_f.text)\n",
    "len_estimate = []\n",
    "for x in sentence_all:\n",
    "    len_estimate.append(len(x))\n",
    "\n",
    "MAX_LEN = int(np.mean(len_estimate)) # Restrict the length of a comment into fixed words number\n",
    "\n",
    "# Use padding to uniform the sentences into same length\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=MAX_LEN, padding='post')\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=MAX_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "def model_all(BATCH_SIZE, net_units, model_sel, EPOCHS = 20):\n",
    "    # Instantiate a distribution strategy of tpu in Kaggle\n",
    "    # If you don't want to use Kaggle's TPU, you can ignore these lines.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "    with tpu_strategy.scope():\n",
    "        inputs = tf.keras.Input(shape=(None,), dtype=\"int32\")\n",
    "        x = layers.Embedding(20000, 64)(inputs)\n",
    "        if model_sel is 'GRU':\n",
    "            x = layers.Bidirectional(layers.GRU(net_units, return_sequences=True))(x)\n",
    "            x = layers.Bidirectional(layers.GRU(64))(x)\n",
    "        elif model_sel is 'LSTM':\n",
    "            x = layers.Bidirectional(layers.LSTM(net_units, return_sequences=True))(x)\n",
    "            x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "        else:\n",
    "            x = layers.Bidirectional(layers.GRU(net_units, return_sequences=True))(x)\n",
    "            x = layers.Bidirectional(layers.GRU(64))(x)\n",
    "        x = layers.Dense(32,  kernel_regularizer=keras.regularizers.l2(0.01))(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        outputs = layers.Dense(3, activation='softmax')(x)\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "#         model.summary()\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam' ,metrics='categorical_accuracy')\n",
    "        history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split = 0.2, verbose=0, \n",
    "                            callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', \n",
    "                                                                          patience=3, mode = 'auto')])\n",
    "        loss, accuracy = model.evaluate(X_test, y_test)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 8s 15ms/step - loss: 0.2794 - categorical_accuracy: 0.9083\n",
      "[0.9083333015441895, 'LSTM', 100, 256]\n",
      "375/375 [==============================] - 7s 14ms/step - loss: 0.3046 - categorical_accuracy: 0.9076\n",
      "[0.9075832962989807, 'LSTM', 50, 256]\n",
      "375/375 [==============================] - 8s 15ms/step - loss: 0.3574 - categorical_accuracy: 0.8913\n",
      "[0.8912500143051147, 'LSTM', 20, 256]\n",
      "375/375 [==============================] - 7s 14ms/step - loss: 0.3461 - categorical_accuracy: 0.8958\n",
      "[0.8958333134651184, 'LSTM', 100, 128]\n",
      "375/375 [==============================] - 8s 15ms/step - loss: 0.3426 - categorical_accuracy: 0.9019\n",
      "[0.9019166827201843, 'LSTM', 50, 128]\n",
      "375/375 [==============================] - 7s 14ms/step - loss: 0.2679 - categorical_accuracy: 0.9065\n",
      "[0.906499981880188, 'LSTM', 20, 128]\n",
      "375/375 [==============================] - 7s 13ms/step - loss: 0.2745 - categorical_accuracy: 0.9032\n",
      "[0.903166651725769, 'LSTM', 100, 64]\n",
      "375/375 [==============================] - 7s 13ms/step - loss: 0.3178 - categorical_accuracy: 0.9039\n",
      "[0.9039166569709778, 'LSTM', 50, 64]\n",
      "375/375 [==============================] - 7s 13ms/step - loss: 0.2880 - categorical_accuracy: 0.9147\n",
      "[0.9146666526794434, 'LSTM', 20, 64]\n",
      "375/375 [==============================] - 7s 15ms/step - loss: 0.3064 - categorical_accuracy: 0.9119\n",
      "[0.9119166731834412, 'GRU', 100, 256]\n",
      "375/375 [==============================] - 7s 15ms/step - loss: 0.2811 - categorical_accuracy: 0.9123\n",
      "[0.9123333096504211, 'GRU', 50, 256]\n",
      "375/375 [==============================] - 7s 15ms/step - loss: 0.2771 - categorical_accuracy: 0.9099\n",
      "[0.9099166393280029, 'GRU', 20, 256]\n",
      "375/375 [==============================] - 7s 14ms/step - loss: 0.2946 - categorical_accuracy: 0.9112\n",
      "[0.9111666679382324, 'GRU', 100, 128]\n",
      "375/375 [==============================] - 7s 14ms/step - loss: 0.2641 - categorical_accuracy: 0.9116\n",
      "[0.9115833044052124, 'GRU', 50, 128]\n",
      "375/375 [==============================] - 7s 14ms/step - loss: 0.3509 - categorical_accuracy: 0.9045\n",
      "[0.9045000076293945, 'GRU', 20, 128]\n",
      "375/375 [==============================] - 7s 14ms/step - loss: 0.2602 - categorical_accuracy: 0.9137\n",
      "[0.9136666655540466, 'GRU', 100, 64]\n",
      "375/375 [==============================] - 7s 13ms/step - loss: 0.2693 - categorical_accuracy: 0.9093\n",
      "[0.909333348274231, 'GRU', 50, 64]\n",
      "375/375 [==============================] - 7s 14ms/step - loss: 0.3126 - categorical_accuracy: 0.9107\n",
      "[0.9107499718666077, 'GRU', 20, 64]\n"
     ]
    }
   ],
   "source": [
    "model_sele_all = ['LSTM', 'GRU']\n",
    "batch_size_all = [100, 50, 20]\n",
    "net_units_all = [256, 128, 64]\n",
    "res_search = []\n",
    "for model in model_sele_all:\n",
    "    for net_units in net_units_all:\n",
    "        for batch_size in batch_size_all:\n",
    "            acc = model_all(batch_size, net_units, model)\n",
    "            print([acc, model, batch_size, net_units])\n",
    "            res_search.append([acc, model, batch_size, net_units])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
