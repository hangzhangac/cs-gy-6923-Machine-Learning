{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate\n",
    "## We recommend you to reach above websites and click the \"new notebook\" inside, turn on the TPU accelerator. The Kaggle enviromnet would be preferable to run our code. If you want to run our code locally, please download the dataset from above website, and change the first few lines in code to refer to your directory with local dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/60k-stack-overflow-questions-with-quality-rate/valid.csv\n",
      "/kaggle/input/60k-stack-overflow-questions-with-quality-rate/train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HQ          15000\n",
      "LQ_EDIT     15000\n",
      "LQ_CLOSE    15000\n",
      "Name: Y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "stack_data1 = pd.read_csv(r'/kaggle/input/60k-stack-overflow-questions-with-quality-rate/train.csv')\n",
    "stack_data2 = pd.read_csv(r'/kaggle/input/60k-stack-overflow-questions-with-quality-rate/valid.csv')\n",
    "print(stack_data1['Y'].value_counts())\n",
    "stack_data = pd.concat([stack_data1,stack_data2])\n",
    "stack_data_f = stack_data.dropna(subset=['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_data_f['text'] = stack_data_f.Title+': '+stack_data_f.Body\n",
    "\n",
    "# if the character in text not from a-z, A-Z replace it with empty\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^(a-zA-Z)\\s]','', text)\n",
    "    return text\n",
    "stack_data_f.text = stack_data_f.text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1): 0.87625\n",
      "MultinomialNB(): 0.78075\n",
      "DecisionTreeClassifier(): 0.7945\n",
      "RandomForestClassifier(): 0.8325833333333333\n"
     ]
    }
   ],
   "source": [
    "#Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(stack_data_f.text, stack_data_f.Y, test_size=0.2, random_state=0 )\n",
    "\n",
    "# Preparation of classifiers\n",
    "classifiers = [\n",
    "    LogisticRegression(C=1),\n",
    "    MultinomialNB(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier()]\n",
    "\n",
    "# Find the best classifier as out baseline\n",
    "for cls in classifiers:\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', TfidfVectorizer(ngram_range=(1,1))),\n",
    "        ('clf', cls)])\n",
    "    text_clf.fit(X_train, y_train)\n",
    "    print(str(cls) +': ' + str(text_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since we discovered that the Logistic Regression has the best performance among 4 classifiers. We then use GridSearch to find the corresponding best hyperparameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2, 'penalty': 'l1', 'solver': 'saga'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GridSearch(cls, parameters, X, y):    \n",
    "    results = pd.DataFrame()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "    \n",
    "    for ind,par in enumerate(list(ParameterGrid(parameters))):\n",
    "        text_clf = Pipeline([\n",
    "                ('vect', TfidfVectorizer()),\n",
    "                ('clf', classifier(**par))])\n",
    "        text_clf.fit(X_train, y_train)\n",
    "        predicted = text_clf.predict(X_test)\n",
    "        results.loc[str(par),'results'] = text_clf.score(X_test, y_test)\n",
    "        results.loc[str(par),'parameters'] = ind\n",
    "        ind_best = results.sort_values(by=['results'], ascending=False).iloc[0,1]\n",
    "\n",
    "    return list(ParameterGrid(parameters))[int(ind_best)]\n",
    "\n",
    "classifier = LogisticRegression\n",
    "parameters = {\n",
    "    'solver':['saga'],\n",
    "    'C': [1, 1.5, 2],\n",
    "    'penalty': ['l1', 'l2']\n",
    " }\n",
    "\n",
    "results = GridSearch(classifier, parameters, stack_data_f.text, stack_data_f.Y)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8925"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(**results)\n",
    "\n",
    "text_clf = Pipeline([\n",
    "                ('vect', TfidfVectorizer()),\n",
    "                ('clf', classifier)])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "text_clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
